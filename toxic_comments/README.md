# Определение токсичных комментариев

# Цель

Необходимо построить модель, которая будет определять токсичные комментарии. Со значением метрики качества F1 не меньше 0.75.  

# Вывод

 - Были выбраны 2 быстрые модели для обучения.
 - Логистическая регрессия показала хороший результат f1 = 0.76.
 - LightGBMClassifier показала себя лучше на валидационной выборке f1 = 0.77.
 - Была достигнута F1 = 0.76 на тестовой выборке, с помощью модели LightGBM
 - ROC-AUC на тестовой выборке 0,96

# Стек

pandas, numpy, nltk, re, matplotlib, sklearn, lightgbm

# Статус

Завершен. Возможны доработки оформления и оптимизации кода.
